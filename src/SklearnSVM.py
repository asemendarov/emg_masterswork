import numpy as np
import matplotlib.pyplot as plt
import sklearn.svm as svm

# Для красоты изменяем стиль графиков
plt.style.use('bmh')

# Задаем координаты точек по двум осям: x1 и x2
x1 = np.array([1, 5, 1.5, 8, 1.1, 9])
x2 = np.array([2, 8, 1.8, 8, 0.6, 11])

# Создаем массив точек - X и массив классов - y
X = np.array([x1, x2]).transpose()
y = np.array([0, 1, 0, 1, 0, 1])

# Определим наш классификатор, где
# kernel - ядро классификатора (в нашем случаи он линейный)
# C = 1.0 - параметр регуляризации SVM
clf = svm.SVC(kernel='linear', C=1.0)

# Обучаем классификатор
clf.fit(X, y)

# Определяем функцию оптимально разделяющая гиперплоскость, где
# coef_ - веса, присвоенные признакам (коэффициенты в основной задаче)
# intercept_ - константы в функции принятия решения
# Прямая описывается функцией: y = a * x - b
w = clf.coef_[0]  # [0] <- т.к. coef_ = [[..,..]]
i = clf.intercept_

a = -w[0] / w[1]
b = i[0] / w[1]

xx = np.linspace(0, 12)
yy = a * xx - b

# Выводим оптимально разделяющую гиперплоскость на график
plt.plot(xx, yy, 'k-', label="Оптимально разделяющая гиперплоскость")

# Выводим точки на график
plt.scatter(X[:, 0], X[:, 1], c=y)

#  Добавим легенду на график и выведем на дисплей
plt.legend()
plt.show()
